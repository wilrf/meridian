---
title: "Linear Regression"
phase: 5
order: 1
requires: ["numpy", "scikit-learn"]
prev: "4.2_pandas"
next: null
---

# Lesson 3: Linear Regression

## The Problem

You notice that when interest rates go up, stock prices tend to go down. You want to quantify this relationship: "For every 1% increase in rates, stocks drop X%." And you want to use this relationship to make predictions.

---

## The Metaphor

Imagine plotting points on a graph—each point is a day with its interest rate (x-axis) and stock return (y-axis). The points scatter but show a general trend.

**Linear regression** is finding the **best-fit line** through those points. The line summarizes the relationship: its slope tells you "how much Y changes when X changes by 1."

The "best-fit" line minimizes the total distance between the line and all the points—it's the line that makes the smallest overall prediction error.

---

## The Technical Reality

### The Linear Model

```
y = β₀ + β₁x + ε

Where:
- y is what we're predicting (dependent variable)
- x is what we're using to predict (independent variable)
- β₀ is the intercept (y when x=0)
- β₁ is the slope (change in y per unit change in x)
- ε is the error (what we can't explain)
```

For multiple variables:
```
y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
```

### How It Learns

The algorithm finds β values that minimize **Mean Squared Error (MSE)**:

```
MSE = (1/n) Σ(yᵢ - ŷᵢ)²

Where ŷᵢ is the predicted value and yᵢ is the actual value
```

This is called **Ordinary Least Squares (OLS)**—it finds the line that minimizes the sum of squared distances from points to the line.

---

## The Code

### Simple Linear Regression (NumPy)

~~~python runnable
import numpy as np
import matplotlib.pyplot as plt

# Generate sample data: stock returns vs interest rate changes
np.random.seed(42)
n = 100
interest_rate_change = np.random.randn(n) * 0.5  # % change in rates
# Returns have negative relationship + noise
stock_returns = -2 * interest_rate_change + np.random.randn(n) * 1

# Fit line using NumPy
# y = mx + b
# Slope: m = Σ(x-x̄)(y-ȳ) / Σ(x-x̄)²
# Intercept: b = ȳ - m*x̄

x = interest_rate_change
y = stock_returns

x_mean = np.mean(x)
y_mean = np.mean(y)

slope = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean)**2)
intercept = y_mean - slope * x_mean

print(f"Slope: {slope:.3f}")
print(f"Intercept: {intercept:.3f}")
# For every 1% increase in interest rates, stocks drop ~2%

# Make predictions
y_pred = slope * x + intercept

# Visualize
plt.scatter(x, y, alpha=0.5, label="Data")
plt.plot(x, y_pred, color="red", label=f"y = {slope:.2f}x + {intercept:.2f}")
plt.xlabel("Interest Rate Change (%)")
plt.ylabel("Stock Return (%)")
plt.legend()
plt.title("Stock Returns vs Interest Rate Changes")
plt.show()
~~~

### Using scikit-learn

~~~python runnable
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd

# Prepare data
X = interest_rate_change.reshape(-1, 1)  # sklearn needs 2D array
y = stock_returns

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Model parameters
print(f"Slope (coefficient): {model.coef_[0]:.3f}")
print(f"Intercept: {model.intercept_:.3f}")

# Make predictions
y_pred = model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Root Mean Squared Error: {rmse:.3f}")
print(f"R² Score: {r2:.3f}")  # 1.0 = perfect, 0 = no better than mean
~~~

### Multiple Linear Regression

~~~python runnable
# Predict stock returns using multiple factors
data = pd.DataFrame({
    "interest_rate_change": np.random.randn(200) * 0.5,
    "unemployment_change": np.random.randn(200) * 0.3,
    "gdp_growth": np.random.randn(200) * 0.2 + 0.5,
    "inflation": np.random.randn(200) * 0.3 + 2,
})

# Create target (stock returns influenced by all factors)
data["stock_return"] = (
    -2 * data["interest_rate_change"] +
    -1 * data["unemployment_change"] +
    1.5 * data["gdp_growth"] +
    -0.5 * data["inflation"] +
    np.random.randn(200) * 1
)

# Features and target
features = ["interest_rate_change", "unemployment_change", "gdp_growth", "inflation"]
X = data[features]
y = data["stock_return"]

# Split and train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = LinearRegression()
model.fit(X_train, y_train)

# View coefficients
for feature, coef in zip(features, model.coef_):
    print(f"{feature}: {coef:.3f}")

# Evaluate
y_pred = model.predict(X_test)
print(f"\nR² Score: {r2_score(y_test, y_pred):.3f}")
~~~

### Feature Scaling

~~~python runnable
from sklearn.preprocessing import StandardScaler

# When features have different scales, scaling helps
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit on training data
X_test_scaled = scaler.transform(X_test)        # Apply same transformation

model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Now coefficients show relative importance
# (larger magnitude = more influential feature)
for feature, coef in zip(features, model.coef_):
    print(f"{feature}: {coef:.3f}")
~~~

### Cross-Validation

~~~python runnable
from sklearn.model_selection import cross_val_score

# More robust evaluation: test on multiple different splits
model = LinearRegression()
scores = cross_val_score(model, X, y, cv=5, scoring="r2")

print(f"R² scores across 5 folds: {scores}")
print(f"Mean R²: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
~~~

### Regularization (Ridge and Lasso)

~~~python runnable
from sklearn.linear_model import Ridge, Lasso

# Ridge: penalizes large coefficients (L2 regularization)
ridge = Ridge(alpha=1.0)  # Higher alpha = more regularization
ridge.fit(X_train, y_train)

# Lasso: can zero out coefficients (L1 regularization)
# Good for feature selection
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

print("Lasso coefficients (zeros = eliminated features):")
for feature, coef in zip(features, lasso.coef_):
    print(f"  {feature}: {coef:.3f}")
~~~

---

## The Exercise

### Exercise 3.1: Predict Stock Prices
~~~python exercise id="5.1.1"
# Use historical data to predict tomorrow's price
# Features: today's price, 5-day moving average, 20-day moving average, volume

def prepare_features(prices: pd.Series) -> pd.DataFrame:
    """
    Create feature DataFrame from price series.
    Include: price, MA5, MA20, volume, returns
    """
    pass

def train_price_model(df: pd.DataFrame) -> LinearRegression:
    """
    Train model to predict next day's return.
    Return the trained model.
    """
    pass
~~~

### Exercise 3.2: Evaluate Model Quality
~~~python exercise id="5.1.2"
# For your price prediction model:
# 1. Calculate R², MSE, MAE
# 2. Plot predicted vs actual returns
# 3. Plot residuals (errors) - should be random, centered at 0
# 4. Calculate how often the model predicts the correct direction (up/down)
~~~

### Exercise 3.3: Feature Importance
~~~python exercise id="5.1.3"
# Given a model with many features:
# 1. Use Lasso to identify most important features
# 2. Retrain with only important features
# 3. Compare performance: all features vs selected features
~~~

---

## The Gotchas

### Gotcha 1: Data Leakage

```python
# WRONG - scaler sees test data!
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Fits on ALL data
X_train, X_test = train_test_split(X_scaled, ...)

# RIGHT - fit only on training data
X_train, X_test = train_test_split(X, ...)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Only transform, don't fit
```

### Gotcha 2: Time Series Splitting

```python
# WRONG for time series - can't use future to predict past!
train_test_split(X, y, random_state=42)  # Random shuffle

# RIGHT - maintain time order
split_idx = int(len(X) * 0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

# Or use TimeSeriesSplit
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
```

### Gotcha 3: Overfitting

```python
# If training R² is high but test R² is low, you're overfitting
print(f"Train R²: {r2_score(y_train, model.predict(X_train)):.3f}")
print(f"Test R²: {r2_score(y_test, model.predict(X_test)):.3f}")

# Solutions:
# - More data
# - Fewer features
# - Regularization (Ridge/Lasso)
# - Cross-validation to detect it
```

### Gotcha 4: R² Can Be Negative

```python
# R² < 0 means model is worse than predicting the mean!
# This happens when model is very bad or wrong test data
r2 = r2_score(y_test, y_pred)
if r2 < 0:
    print("Model is worse than baseline - something is wrong")
```

---

## Key Takeaways

1. Linear regression finds the best-fit line through data
2. Coefficients tell you how much Y changes per unit X
3. R² measures how much variance the model explains (0 to 1)
4. Always split data into train/test to evaluate fairly
5. Scale features when they have different units
6. Use regularization (Ridge/Lasso) to prevent overfitting
7. For time series, maintain chronological order in splits

---

## Next Lesson

Linear regression assumes a straight-line relationship. Real-world data is messier. Next: **Introduction to scikit-learn**—the full machine learning toolkit.
